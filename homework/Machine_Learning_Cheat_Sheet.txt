
* SVM
支撐向量機(support vector machine, SVM)

* CART: 

* k-NN 分類器:  k-Nearest Neighbors 

------

# Day 36
	* 評估指標 - 回歸
		* MAE, Mean Absolute Error, 範圍: [-∞, ∞]
		* MSE, Mean Square Error, 範圍: [-∞, ∞]
		* R-square, 範圍: [0, 1]
	* 評估指標 - 分類
		* AUC, Area Under Curve, 範圍: [0, 1]
			* The curve is : 接收者操作特徵曲線（receiver operating characteristic curve，或者叫ROC曲線）
		* F1-Score
			* F1-Score 則是 Precision, Recall 的調和平均數
			* Precision: 模型判定瑕疵，樣本確實為瑕疵的比例例
				* Precision = TP / (TP + FP)
			* Recall: 模型判定的瑕疵，佔樣本所有瑕疵的比例例
				* Recall = TP / (TP + FN)
			* Definitions
				* 真陽性 (TP, true positive)
					* 正確的肯定。又稱：命中 (hit)
				* 真陰性 (TN, true negative)
					* 正確的否定。又稱：正確拒絕 (correct rejection)
				* 偽陽性 (FP, false positive)
					* 錯誤的肯定，又稱：假警報 (false alarm)，第一型錯誤
				* 偽陰性 (FN, false negative)
					* 錯誤的否定，又稱：未命中 (miss)，第二型錯誤
	* 混淆矩陣 (Confusion Matrix) ?
	* Q: 這麼多評估指標，該怎麼選擇?
		* 回歸問題: 可以透過 R-square 很快了了解預測的準確程度；
		* 分類問題: 若若為二分類 (binary classification)，通常使用 AUC 評估。
			* 但如果有特別希望哪一類別不要分錯，則可使用 F1-Score，觀察 Recall 值或是 Precision 值。
		* 多分類問題: 則可使用 top-k accuracy，k 代表模型預測前 k 個類別有包含正確類別即為正確 (ImageNet 競賽通常都是比 Top-5 Accuracy)
		
		
------

# Day 37 Regression 模型

	* 把 LogisticRegression 訓練出來的結果視覺化
```
	lr = LogisticRegression
	lr.fit(X_train_std, y_train['target'].values)
	
	plot_decision_regions(X_train_std, y_train['target'], classifier=lr)
	plt.xlabel('...')
	plt.ylabel('...')
	plt.show()
```

	* Logistic Regression優點：
		* 資料不需要線性可分
		* 可以獲得A類跟B類的機率
		* 實務上Logistic Regression執行速度非常快
		
	* Logistic Regression缺點：
		* 線的切法不夠漂亮，以人的觀察應該要大概要像是綠色的線才是一個比較好的分法（下一章的SVM將會解決這個問題）





------

# Day 41

	* 吉尼係數 (gini-index) 或熵 (entropy)
		* 0 => 1 
			* 1 - (1/2)**2 = 1/2  
			
	* 混亂程度有兩種評估方式：
		一、 Gini impurity ：各個類別的數據比例，兩兩相乘（不乘自身），總和越小越好；
		二、 information gain ：各個類別的數據比例，求熵，總和越小越好。
		
	* Decision Tree
		* 把區隔效果 (Information Gain) 最好的放在跟目錄
		* 以公司要不要錄取來說, 考試是不是及格, 就是最大的因素 (以我個人來說). 如果考 0 分, 100% 不錄取
		* 以要不要處理 email, 收件者有提到我的名字的, 就優先處理
		
	
------

# Day 42

## 建立模型四步驟

	* 在 Scikit-learn 中，建立一個機器學習的模型其實非常簡單，流程大略是以下四個步驟

	* 讀進資料，並檢查資料的 shape (有多少 samples (rows), 多少 features (columns)，label 的型態是什麼？)
		* 使用 pandas 讀取 .csv 檔：pd.read_csv
		* 使用 numpy 讀取 .txt 檔：np.loadtxt
		* 使用 Scikit-learn 內建的資料集：sklearn.datasets.load_xxx
		* 檢查資料數量：data.shape (data should be np.array or dataframe)
		
	* 將資料切為訓練 (train) / 測試 (test)
		* train_test_split(data)
		
	* 建立模型，將資料 fit 進模型開始訓練
		* clf = DecisionTreeClassifier()
		* clf.fit(x_train, y_train)
		
	* 將測試資料 (features) 放進訓練好的模型中，得到 prediction，與測試資料的 label (y_test) 做評估
		* clf.predict(x_test)
		* accuracy_score(y_test, y_pred)
		* f1_score(y_test, y_pred)
	
	* print(iris.feature_names)
	* print("Feature importance: ", clf.feature_importances_)
	
------

# Day 43

	* One of 整體學習（Ensemble learning）
		* 可以將數個分類器的預測結果綜合考慮，藉此達到顯著提升分類效果

	* Bootstrap Aggregating


------

# Day 44

	------

	 ## sklearn.ensemble.RandomForestClassifier
	```
	class sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)[source]
	```
	A random forest classifier.

	A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).

	Read more in the User Guide.

	### Parameters:	
	#### n_estimators : integer, optional (default=10)
	The number of trees in the forest.

	Changed in version 0.20: The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22.

	#### criterion : string, optional (default=”gini”)
	The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.

	#### max_depth : integer or None, optional (default=None)
	The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.

	#### min_samples_split : int, float, optional (default=2)
	The minimum number of samples required to split an internal node:

	If int, then consider min_samples_split as the minimum number.
	If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
	Changed in version 0.18: Added float values for fractions.

	#### min_samples_leaf : int, float, optional (default=1)
	The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.

	If int, then consider min_samples_leaf as the minimum number.
	If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
	Changed in version 0.18: Added float values for fractions.

	#### min_weight_fraction_leaf : float, optional (default=0.)
	The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.

	#### max_features : int, float, string or None, optional (default=”auto”)
	The number of features to consider when looking for the best split:

	If int, then consider max_features features at each split.
	If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.
	If “auto”, then max_features=sqrt(n_features).
	If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).
	If “log2”, then max_features=log2(n_features).
	If None, then max_features=n_features.
	Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.

	#### max_leaf_nodes : int or None, optional (default=None)
	Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.

	#### min_impurity_decrease : float, optional (default=0.)
	A node will be split if this split induces a decrease of the impurity greater than or equal to this value.

	The weighted impurity decrease equation is the following:

	N_t / N * (impurity - N_t_R / N_t * right_impurity
						- N_t_L / N_t * left_impurity)
	where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.

	N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.

	New in version 0.19.

	#### min_impurity_split : float, (default=1e-7)
	Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.

	Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split will change from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead.
	#### bootstrap : boolean, optional (default=True)
	Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.

	#### oob_score : bool (default=False)
	Whether to use out-of-bag samples to estimate the generalization accuracy.

	#### n_jobs : int or None, optional (default=None)
	The number of jobs to run in parallel for both fit and predict. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.

	#### random_state : int, RandomState instance or None, optional (default=None)
	If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.

	#### verbose : int, optional (default=0)
	Controls the verbosity when fitting and predicting.

	#### warm_start : bool, optional (default=False)
	When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See the Glossary.

	#### class_weight : dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None)
	Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.

	Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].

	The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))

	The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.

	For multi-output, the weights of each column of y will be multiplied.

	Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.

	### Attributes:	
	#### estimators_ : list of DecisionTreeClassifier
	The collection of fitted sub-estimators.

	#### classes_ : array of shape = [n_classes] or a list of such arrays
	The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).

	#### n_classes_ : int or list
	The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).

	#### n_features_ : int
	The number of features when fit is performed.

	#### n_outputs_ : int
	The number of outputs when fit is performed.

	#### feature_importances_ : array of shape = [n_features]
	Return the feature importances (the higher, the more important the feature).

	#### oob_score_ : float
	Score of the training dataset obtained using an out-of-bag estimate.

	#### oob_decision_function_ : array of shape = [n_samples, n_classes]
	Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, oob_decision_function_ might contain NaN.

	------

	=============


	-------

	## sklearn.ensemble.RandomForestRegressor
	```
	class sklearn.ensemble.RandomForestRegressor(n_estimators=’warn’, criterion=’mse’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)[source]
	A random forest regressor.
	```

	A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).

	Read more in the User Guide.

	### Parameters:	
	#### n_estimators : integer, optional (default=10)
	The number of trees in the forest.

	Changed in version 0.20: The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22.

	#### criterion : string, optional (default=”mse”)
	The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error.

	New in version 0.18: Mean Absolute Error (MAE) criterion.

	#### max_depth : integer or None, optional (default=None)
	The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.

	#### min_samples_split : int, float, optional (default=2)
	The minimum number of samples required to split an internal node:

	If int, then consider min_samples_split as the minimum number.
	If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
	Changed in version 0.18: Added float values for fractions.

	min_samples_leaf : int, float, optional (default=1)
	The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.

	If int, then consider min_samples_leaf as the minimum number.
	If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
	Changed in version 0.18: Added float values for fractions.

	#### min_weight_fraction_leaf : float, optional (default=0.)
	The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.

	#### max_features : int, float, string or None, optional (default=”auto”)
	The number of features to consider when looking for the best split:

	If int, then consider max_features features at each split.
	If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.
	If “auto”, then max_features=n_features.
	If “sqrt”, then max_features=sqrt(n_features).
	If “log2”, then max_features=log2(n_features).
	If None, then max_features=n_features.
	Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.

	#### max_leaf_nodes : int or None, optional (default=None)
	Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.

	#### min_impurity_decrease : float, optional (default=0.)
	A node will be split if this split induces a decrease of the impurity greater than or equal to this value.

	The weighted impurity decrease equation is the following:

	N_t / N * (impurity - N_t_R / N_t * right_impurity
						- N_t_L / N_t * left_impurity)
	where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.

	N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.

	New in version 0.19.

	#### min_impurity_split : float, (default=1e-7)
	Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.

	Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split will change from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead.
	bootstrap : boolean, optional (default=True)
	Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.

	#### oob_score : bool, optional (default=False)
	whether to use out-of-bag samples to estimate the R^2 on unseen data.

	#### n_jobs : int or None, optional (default=None)
	The number of jobs to run in parallel for both fit and predict. None` means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.`

	#### random_state : int, RandomState instance or None, optional (default=None)
	If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.

	#### verbose : int, optional (default=0)
	Controls the verbosity when fitting and predicting.

	#### warm_start : bool, optional (default=False)
	When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See the Glossary.

	### Attributes:	
	#### estimators_ : list of DecisionTreeRegressor
	The collection of fitted sub-estimators.

	#### feature_importances_ : array of shape = [n_features]
	Return the feature importances (the higher, the more important the feature).

	#### n_features_ : int
	The number of features when fit is performed.

	#### n_outputs_ : int
	The number of outputs when fit is performed.

	#### oob_score_ : float
	Score of the training dataset obtained using an out-of-bag estimate.

	#### oob_prediction_ : array of shape = [n_samples]
	Prediction computed with out-of-bag estimate on the training set.

	-------



	from sklearn import datasets, metrics
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.ensemble import RandomForestRegressor
	from sklearn.model_selection import train_test_split
	
	class RandomForestClassifierEvaluator():
		def __init__(self, n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None):
			
			# 建立模型
			self.clf = RandomForestClassifier(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, min_impurity_split)
		
		def run(self, dataset):

			# 切分訓練集/測試集
			x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=4)


			# 訓練模型
			self.clf.fit(x_train, y_train)

			# 預測測試集
			y_pred = self.clf.predict(x_test)

			acc = metrics.accuracy_score(y_test, y_pred)

			print("Acuuracy: ", acc)

			if 'feature_names' in dataset:
			
				print(dataset.feature_names)
				print("Feature importance: ", self.clf.feature_importances_)
			
	# 讀取鳶尾花資料集
	iris = datasets.load_iris()
	evaluator = RandomForestClassifierEvaluator()
	evaluator.run(iris)
			
	from sklearn.metrics import mean_squared_error
	from sklearn.metrics import mean_absolute_error
	
	class RandomForestRegressorEvaluator():
		def __init__(self, n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False):

			# 建立模型
			self.clf = RandomForestRegressor(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, min_impurity_split, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start)
			self.criterion = criterion
			
		def run(self, dataset):
			# 切分訓練集/測試集
			
			x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=4)
			# 訓練模型
			self.clf.fit(x_train, y_train)

			# 預測測試集
			y_pred = self.clf.predict(x_test)

			if self.criterion == 'mae':
				# 預測值與實際值的差距，使用 MAE
				print("\033[1;33m Mean absolute error: %.2f \033[0m" % mean_absolute_error(y_test, y_pred))
			else:
				# 預測值與實際值的差距，使用 MSE
				print("\033[1;33m Mean squared error: %.2f \033[0m" % mean_squared_error(y_test, y_pred))
			
			if 'feature_names' in dataset:
			
				print(dataset.feature_names)
				print("Feature importance: ", self.clf.feature_importances_)
			
	# 讀取 Boston 資料集
	boston = datasets.load_boston()
	evaluator = RandomForestRegressorEvaluator()
	evaluator.run(boston)



# Day 45

	* Ensemble: Gradient Boosting vs. Bagging (Bootstrap Aggregating)
	
	* Bias vs. Variance trade-off
		* Linear model had a higher bias
		* Polynomial model, depends a lot on the choice of training data. 
		* If you change the data slightly, the shape of the curve will look very different, and the error will swing widely. 
		* Therefore, the model is said to have high variance.
		
	* Bias: evaluating under-fitting
	* Variance: evaluating over-fitting
	
	* Bagging:
		* Use Average or Voting to reduce variance to avoid over-fitting
			* Use Average: in Regression problem
			* Use Voting: in Classification problem
			
	* Out of Bag : evaluating error of data out of bag
	
	* Bagging is used with "Strong Models": Use averaging or voting to avoid over-fitting caused by strong models
	* Boosting is used with "Weak Models"
	
	* GBDT(Gradient Boosting Decision Tree) 又叫 MART(Multiple Additive Regression Tree)，
		* 是一種迭代的決策樹算法，該算法由多棵決策樹組成，所有樹的結論累加起來做最終答案。
		* 它在被提出之初就和SVM一起被認為是泛化能力較強的算法。

	* GBDT中的樹是回歸樹(不是分類樹)，GBDT用來做回歸預測，調整後也可以用于分類。
	
	* Bagging算法是這樣做的︰每個分類器都隨機從原樣本中做有放回的采樣，然後分別在這些采樣後的樣本上訓練分類器，然後再把這些分類器組合起來。
		* 簡單的多數投票一般就可以。
		* 其代表算法是隨機森林。
	* Boosting的意思是這樣，他通過迭代地訓練一系列的分類器，每個分類器采用的樣本分布都和上一輪的學習結果有關。其代表算法是AdaBoost, GBDT。
	
	* 對于Bagging算法來說，由于我們會並行地訓練很多不同的分類器的目的就是降低這個方差(variance) ,
		* 因為采用了相互獨立的基分類器多了以後，h的值自然就會靠近.所以對于每個基分類器來說，
		* 目標就是如何降低這個偏差(bias),所以我們會采用深度很深甚至不剪枝的決策樹。

	* 對于Boosting來說，每一步我們都會在上一輪的基礎上更加擬合原資料，所以可以保證偏差(bias),
		* 所以對于每個基分類器來說，問題就在于如何選擇variance更小的分類器，即更簡單的分類器，所以我們選擇了深度很淺的決策樹。
		
	* Erwin: 
		* Bagging 是用一堆 over-fitting 的 tree (深度很深)來組合, 避免 over-fitting ?
		* Boosting 是用一堆 under-fitting 的 tree (深度很深)來組合, 避免 under-fitting ?
			* 迭代 (iteration) 多棵回歸樹來共同決策
		
	* xgboost 的全稱是eXtreme Gradient Boosting，它是Gradient Boosting Machine的一個c++實現，作者為正在華盛頓大學研究機器學習的大牛陳天奇 。
		* xgboost，在計算速度和準確率上，較GBDT有明顯的提升。
		* xgboost最大的特點在于，它能夠自動利用CPU的多線程進行並行，同時在算法上加以改進提高了精度。
		* 它的處女秀是Kaggle的 希格斯子信號識別競賽，因為出眾的效率與較高的預測準確度在比賽論壇中引起了參賽選手的廣泛關注。
		

# Day 46

	* Sample codes
		from sklearn.ensemble import GradientBoostingRegressor
		from sklearn.ensemble import GradientBoostingClassifier
		
		clf = GradientBoostingClassifier(
			loss="deviance", #Loss 的選擇，若若改為 exponential 則會變成
			Adaboosting 演算法，概念念相同但實作稍微不同
			learning_rate=0.1, #每棵樹對最終結果的影響，應與 n_estimators 成反比
			n_estimators=100 #決策樹的數量量
		)
		
	* Class Wrapper

		from sklearn.ensemble import GradientBoostingClassifier
		from sklearn.ensemble import GradientBoostingRegressor
		from sklearn.model_selection import train_test_split

		class GradientBoostingClassifierEvaluator():
			def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001):

				# 建立模型
				self.clf = GradientBoostingClassifier(loss, learning_rate, n_estimators, subsample, criterion, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_decrease, min_impurity_split, init, random_state, max_features, verbose, max_leaf_nodes, warm_start, presort, validation_fraction, n_iter_no_change, tol)

			def run(self, dataset):

				# 切分訓練集/測試集
				x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=4)


				# 訓練模型
				self.clf.fit(x_train, y_train)

				# 預測測試集
				y_pred = self.clf.predict(x_test)

				acc = metrics.accuracy_score(y_test, y_pred)

				print("Acuuracy: ", acc)

				if 'feature_names' in dataset:
					
					print(dataset.feature_names)
					print("Feature importance: ", self.clf.feature_importances_)
					
		
		evaluator = GradientBoostingClassifierEvaluator()
		evaluator.run(digits)
		Acuuracy:  0.9644444444444444
		
# Day 47

	* Sample codes
		from sklearn import datasets, metrics
		from sklearn.model_selection import train_test_split, KFold, GridSearchCV
		from sklearn.ensemble import GradientBoostingRegressor


		# 讀取手寫辨識資料集
		digits = datasets.load_digits()
		# 切分訓練集/測試集
		x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=42)

		# 建立模型
		clf = GradientBoostingRegressor(random_state=7)

		# 先看看使用預設參數得到的結果，約為 1.41 的 MSE
		clf.fit(x_train, y_train)
		y_pred = clf.predict(x_test)
		print(metrics.mean_squared_error(y_test, y_pred))

		# 設定要訓練的超參數組合
		n_estimators = [100, 200, 300, 500, 1000, 3000, 10000]
		max_depth = [1, 3, 5, 7]

		# 建立 dictionary
		param_grid = dict(n_estimators=n_estimators, max_depth=max_depth)

		## 建立搜尋物件，放入模型及參數組合字典 (n_jobs=-1 會使用全部 cpu 平行運算)
		grid_search = GridSearchCV(clf, param_grid, scoring="neg_mean_squared_error", n_jobs=-1, verbose=1)

		# 開始搜尋最佳參數
		grid_result = grid_search.fit(x_train, y_train)

		# 印出最佳結果與最佳參數
		print("Best Accuracy: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

		# 使用最佳參數重新建立模型
		clf_bestparam = GradientBoostingRegressor(max_depth=grid_result.best_params_['max_depth'],
												   n_estimators=grid_result.best_params_['n_estimators'])

		# 訓練模型
		clf_bestparam.fit(x_train, y_train)

		# 預測測試集
		y_pred = clf_bestparam.predict(x_test)

		# 調整參數後約可降至 0.96 的 MSE
		print(metrics.mean_squared_error(y_test, y_pred))

	* But in truth even a random search of the parameter space can be MUCH more effective than a grid search!
	
	
# Day 048

	* Sample 1 GridSearchCV
	
		from sklearn.model_selection import train_test_split, KFold, GridSearchCV

		# 設定要訓練的超參數組合
		n_estimators = [10,50, 100, 200]
		max_depth = [1, 3, 5, 7, 10]

		# 建立 dictionary
		param_grid = dict(n_estimators=n_estimators, max_depth=max_depth)

		## 建立搜尋物件，放入模型及參數組合字典 (n_jobs=-1 會使用全部 cpu 平行運算)
		grid_search = GridSearchCV(clf, param_grid, scoring="f1", n_jobs=-1, verbose=1)

		# 開始搜尋最佳參數
		grid_result = grid_search.fit(x_train, y_train)

		# 印出最佳結果與最佳參數
		print("Best Accuracy: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

		# 使用最佳參數重新建立模型
		clf_bestparam = GradientBoostingClassifier(max_depth=grid_result.best_params_['max_depth'],
												   n_estimators=grid_result.best_params_['n_estimators'])

		# 訓練模型
		clf_bestparam.fit(x_train, y_train)

		# 預測測試集
		y_pred = clf_bestparam.predict(x_test)

		# 調整參數後約可降至 0.96 的 MSE
		print(metrics.mean_squared_error(y_test, y_pred))

	* Sample 2 
	
		from sklearn.model_selection import RandomizedSearchCV
		
		param_dict = {
				'n_estimators':range(10,500,4),
				'max_depth':range(2,15,1),
				'learning_rate':np.linspace(0.001, 0.01, 2, 20),
				'subsample':np.linspace(0.5, 0.95, 20)
				}



		## 建立搜尋物件，放入模型及參數組合字典 (n_jobs=-1 會使用全部 cpu 平行運算)
		random_search = RandomizedSearchCV(clf, param_dict, scoring="f1", n_jobs=-1, verbose=1)

		# 開始搜尋最佳參數
		random_result = random_search.fit(x_train, y_train)

		# 印出最佳結果與最佳參數
		print("Best Accuracy: %f using %s" % (random_result.best_score_, random_result.best_params_))

		# 使用最佳參數重新建立模型
		clf_bestparam = GradientBoostingClassifier(max_depth=random_result.best_params_['max_depth'],
												   n_estimators=random_result.best_params_['n_estimators'],
												   learning_rate=random_result.best_params_['learning_rate'],
												   subsample=random_result.best_params_['subsample']
												  )

		# 訓練模型
		clf_bestparam.fit(x_train, y_train)

		# 預測測試集
		y_pred = clf_bestparam.predict(x_test)

		# 調整參數後約可降至 0.96 的 MSE
		print(metrics.mean_squared_error(y_test, y_pred))
		


# Day 054 : clustering 1 非監督式機器學習簡介

	* Unsupervised Learning 就是用來做 Clustering 
	

# Day 055 : clustering 2 聚類算法

	* Differences:
		* 監督式學習 目標在於找出決策邊界(decision boundary)
		* Clustering 目標在於找出資料結構(關聯性)
	
	* K-means 聚類算法
		* 把所有資料點分成 k 個 cluster，使得相同 cluster 中的所有資料點彼此儘量量相似，而不同 cluster 的資料點儘量量不同。
		* 距離測量量（e.g. 歐氏距離）用於計算資料點的相似度和相異異度。每個 cluster 有一個中心點。中心點可理理解為最能代表 cluster 的點。
		* K-means 目標是使總體群內平方誤差 (其他點與 Centroid 的距離) 最小
		
	* K-means 注意事項
		* Random initialization: initial 設定的不同，會導致得到不同 clustering 的結果，可能導致 local optima，⽽而非 global optima。
		* 因爲沒有預先的標記，對於 cluster 數量量多少才是最佳解，沒有標準答案，得靠手動測試觀察。
			* 盡可能把資料 Visualize ?
		* 
		
	* Ground True
		* In machine learning, the term "ground truth" refers to the accuracy of the training set's classification 
		* for supervised learning techniques
		
	* Sample codes
		import matplotlib
		import matplotlib.pyplot as plt
		# Though the following import is not directly being used, it is required
		# for 3D projection to work
		from mpl_toolkits.mplot3d import Axes3D

		from sklearn.cluster import KMeans
		from sklearn import datasets
		%matplotlib inline
		
		estimators = [('k_means_8', KMeans(n_clusters=8)),
				  ('k_means_3', KMeans(n_clusters=3)),
				  ('k_means_bad_init', KMeans(n_clusters=3, n_init=10, init='random'))]
		fignum = 1
		titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']
		for name, est in estimators:
			fig = plt.figure(fignum, figsize=(8, 6))
			ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
			
			## fit data
			## est = KMeans(....)
			est.fit(X)
			
			labels = est.labels_
			print (f'labels = {labels}')

			# scatter(xs, ys, zs=0, zdir='z', s=20, c=None, depthshade=True, *args, **kwargs)
			# Create a scatter plot
			# xs, ys : array-like. The data positions.
			#     float or array-like, optional, default: 0
			#     The z-positions. Either an array of the same length as xs and ys or a single value to place all points in the same plane.
			# c : color, sequence, or sequence of color, optional
			ax.scatter(X[:, 3], X[:, 0], X[:, 2],
					   c=labels.astype(np.float), edgecolor='k')

			#ax.w_xaxis.set_ticklabels([])
			ax.set_xlabel('x axis')
			#ax.w_yaxis.set_ticklabels([])
			ax.set_ylabel('y axis')
			#ax.w_zaxis.set_ticklabels([])
			ax.set_zlabel('z axis')
			ax.set_title(titles[fignum - 1])
			ax.dist = 12
			fignum = fignum + 1

			
# Day 056 : clustering 2 聚類算法

	* 輪輪廓分析 (Silhouette analysis)
	
	* 轮廓系数
		* 取值为[-1, 1]，其值越大越好，且当值为负时，表明 ai<bi，样本被分配到错误的簇中，聚类结果不可接受。
		* 对于接近0的结果，则表明聚类结果有重叠的情况

	* sklearn.metrics.silhouette_score(X, labels, metric=’euclidean’, sample_size=None, random_state=None, **kwds)
		* Compute the mean Silhouette Coefficient of all samples
		* Is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. 
		* The Silhouette Coefficient for a sample is (b - a) / max(a, b). 
		* To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of. 
		* Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1
		* The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.
		
	* 可以 draw :
		* cluster center
		* silhouette values (空間分布)
		

# Day 057 : clustering 3 階層分群算法

	* 階層式分析
		* 一種構建 cluster 的層次結構的算法。
		* 該算法從分配給自己 cluster 的所有資料點開始。
		* 然後，兩兩個距離最近的 cluster 合併為同一個 cluster。
		* 最後，當只剩下一個 cluster 時，該算法結束。
		
	* K-means vs. 階層分群
		* K-mean 要預先定義群數 (n of clusters)
		* 階層分群可根據定義距離來來分群(bottom-up)，也可以決定羣數做分羣 (top-down)
		
	* 階層分群演算法流程
		* 不指定分群的數量量
			• 每筆資料為一個 cluster
			• 計算每兩兩群之間的距離
			• 將最近的兩兩群合併成一群
			• 重覆步驟 2、3，直到所有資料合併成同一 cluster
			
	* 階層分群距離計算方式 : single-link
		* 群聚與群聚間的距離可以定義為不同群聚中最接近兩兩點間的距離
		
	* 階層分群距離計算方式 : complete-link
		* 群聚間的距離定義為不同群聚中最遠兩兩點間的距離，這樣可以保證這兩兩個集合合併後, 任何一對的距離不會大於 d。
		
	* 階層分群距離計算方式 : average-link
		* 群聚間的距離定義為不同群聚間各點與各點間距離總和的平均。
		
	* 階層分群優劣分析
		* 優點
			1. 概念簡單，易於呈現
			2. 不需指定群數
		* 缺點
			* 只適用於少量資料，大量資料會很難處理
			
	* 結論
		* 階層式分群在無需定義群數的情況下做資料的分群，而後可以用不同的距離定義方式決定資料群組。
		* 分群距離計算方式有 single-link, complete-link, average-link。
		* 概念簡單且容易呈現，但不適合用在大資料。
		

# Day 059 : clustering 3 階層分群算法

	* datasets.make_moons: interleaving half circles, Complete is worst
	* Isotropic: all clustering methods are good
	* dataset.make_circles(同心圓 ): Complete is worst
	* Anisotropically distributed data(斜三群): Complete is worst
	

# D59：dimension reduction 1 降維方法-主成份分析

	* 爲什什麼需要降低維度 ?
		* 特徵組合及抽象化
			* 壓縮資料可進而組合出新的、抽象化的特徵，減少冗餘的資訊
		* 資料視覺化
			* 特徵太多時，很難 visualize data, 不容易易觀察資料
			* 把資料維度 (特徵) 降到 2 到 3 個 , 則能夠用一般的 2D 或 3D 圖表呈現資料
			
	* Principle Component Analysis (PCA) 主成份分析
		* PCA 透過計算 eigen value, eigen vector, 可以將原本的 features 降維至特定的維度
			• 原本資料有 100 個 features，透過 PCA，可以將這 100 個 features 降成 2 個 features
			• 新 features 為舊 features 的線性組合
			
	* PCA 應用在監督式學習的注意事項
		* 不建議在早期時做 , 否則可能會丟失重要 features 而 underfitting
		* 可以在 optimization 階段時 , 考慮 PCA , 並觀察運用了了 PCA 後對準確度的影響
		
	* 降低維度可以幫助我們壓縮及丟棄無用資訊、抽象化及組合新特徵、呈現高維數據(?)。常用的算法爲主成分分析。
	* 在維度太大發生 overfitting 的情況下，可以嘗試用 PCA 組成的特徵來來做監督式學習，但不建議一開始就做。
	
	
# D60：dimension reduction 1 降維方法-主成份分析

	* PCA + 邏輯斯迴歸判斷手寫辨識資料集,觀察不同 component 下正確率的變化
	
	* SGDClassifier
		* Stochastic Gradient Descent (SGD) learning
		* Linear classifiers (SVM, logistic regression, a.o.) with SGD training.
		* 
		
	* Sample codes
		# 載入套件
		import numpy as np
		import matplotlib.pyplot as plt
		import pandas as pd

		from sklearn import datasets
		from sklearn.decomposition import PCA
		from sklearn.linear_model import SGDClassifier
		from sklearn.pipeline import Pipeline
		from sklearn.model_selection import GridSearchCV
		import warnings
		warnings.filterwarnings("ignore")

		# 定義 PCA 與隨後的羅吉斯迴歸函數
		# penalty 改為 L1, max_iter 改為 100
		logistic = SGDClassifier(loss='log', penalty='l1', max_iter=10000, tol=1e-5, random_state=0)
		pca = PCA()
		pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])

		# 載入手寫數字辨識集
		digits = datasets.load_digits()
		X_digits = digits.data
		y_digits = digits.target

		# 執行 GridSearchCV 跑出最佳參數


		# Note: Pipeline 的特殊用法 "__" 是 delimiter
		#    pca__n_components => Apply "n_components" to "pca"
		#    logistic__alpha => Apply "alpha" to "logistic"
		param_grid = {
			'pca__n_components': [4, 10, 20, 30, 40, 50, 64],
			'logistic__alpha': np.logspace(-4, 4, 5),
		}

		'''
		sklearn.model_selection.GridSearchCV

			Exhaustive search over specified parameter values for an estimator
			
			Important members are fit, predict.

			GridSearchCV implements a “fit” and a “score” method. 
			It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” 
			if they are implemented in the estimator used.
			
			param_grid : dict or list of dictionaries
				Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, 
				or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. 
				This enables searching over any sequence of parameter settings.
			
		'''

		search = GridSearchCV(pipe, param_grid, iid=False, cv=5, return_train_score=False)
		search.fit(X_digits, y_digits)
		print("Best parameter (CV score=%0.3f):" % search.best_score_)
		print(search.best_params_)

		# 繪製不同 components 的 PCA explained variance ratio
		pca.fit(X_digits)

		# Erwin:
		#    sharex: means share x coordinate and labels
		fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(12, 12))

		'''
		Draw ax0
		'''

		print(f'pca.explained_variance_ratio_.shape={pca.explained_variance_ratio_.shape}')
		# print(f'pca.explained_variance_ratio_={pca.explained_variance_ratio_}')

		# explained_variance_ratio_: Percentage of variance explained by each of the selected components.
		ax0.plot(pca.explained_variance_ratio_, linewidth=2)
		ax0.set_ylabel('PCA explained variance')
		ax1.set_xlabel('n_components')

		# axvline : draw vertical line
		ax0.axvline(search.best_estimator_.named_steps['pca'].n_components, linestyle=':', label='n_components chosen')
		ax0.legend(prop=dict(size=12))

		'''
		Draw ax1
		'''


		# 繪製 7 個不同採樣點的 accuracy
		results = pd.DataFrame(search.cv_results_)
		print(f'results.shape = {results.shape}')
		print(f'results.head() = {results.head(10)}')
		components_col = 'param_pca__n_components'

		'''
		pandas.DataFrame.nlargest

			DataFrame.nlargest(n, columns, keep='first')
			
			Return the first n rows ordered by columns in descending order.
		'''
		# Return an Dataframe:  
		#    Grouped by components_col (or 'param_pca__n_components',  or 4,10,20, ...)
		#    Sort and Pick the largest y='mean_test_score'
		#    Returning the dataframe is
		#        x: 7 rows ('param_pca__n_components')
		#        y: 15 columns (all columns from GridSearchCV)
		best_clfs = results.groupby(components_col).apply(lambda d: d.nlargest(1, 'mean_test_score'))
		print(f'type(best_clfs) = {type(best_clfs)}')
		print(f'best_clfs.shape = {best_clfs.shape}')

		best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score', legend=False, ax=ax1)
		ax1.set_ylabel('Classification accuracy (val)')
		ax1.set_xlabel('n_components')
		# plt.tight_layout()

		plt.show()


# D61：dimension reduction 2 降維方法-T-SNE

	* PCA 的問題
		* PCA 是一種線性降維方式，因此若若特徵間是非線性關係，會有 underfitting 的問題。
		* 求共變異異數矩陣進行奇異值分解，因此會被資料的差異性影響，無法很好的表現相似性及分佈。 ?
		
	* t-SNE
		* t-SNE 也是一種降維方式，但它用了更複雜的公式來表達高維和低維之間的關係。
		* 主要是將高維的資料用 Gaussian Distribution 的機率密度函數近似，
		* 而低維資料的部分用 "t分佈" 來近似，再用 KL divergence 計算相似度，
		* 再以梯度下降 (gradient descent) 求最佳解。
			* Gaussian Distribution : 常態分布
			* t分佈
			* KL divergence
			* gradient descent
			
	* t-SNE 優劣
		* 優點
			* 當特徵數量過多時，使用 PCA 可能會造成降維後的 underfitting，這時可以考慮使用t-SNE 來降維
		* 缺點
			* t-SNE 的需要比較多的時間執行
			
	* 重要知識點複習
		* 特徵間爲非線性關係時 (e.g. 文字、影像資料)，PCA 很容易 underfitting
		* t-SNE 對於特徵非線性資料有更好的降維呈現能力
		
	* t-SNE: t-Distributed Stochastic Neighbor Embedding
	
# D62：dimension reduction 2 降維方法-T-SNE

	* t-SNE
		* 觀察S曲線使用 t-SNE 不同 perplexity 的流形還原效果
		
	* Perplexity
		* In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. 
		* It may be used to compare probability models. 
		* A low perplexity indicates the probability distribution is good at predicting the sample
		
	* Perplexity 因形狀而異？
		* Circle : 越高越好
		* S-Curve : 不是越高越好，約 7/8 sample 數量比較合適？
		
# D63：神經網路介紹

	* 不過，深度學習的原理與實作的門檻並不太高，真正的難處不在深度學習本身，
	* 而是在於如何將人類要解決的問題用數字來表達，並設計成機器可以學習的架構，如何用數字來表示貓臉？用數字來描述圍棋？這些都需要人類去定義，
	* 因此深度學習要進一步發展，最需要的其實是人才，剩下的，就是機器的事了。
	
	
# D64：深度學習體驗 : 模型調整與學習曲線

	* 雖然圖像化更直覺，但是並非量量化指標且可視化不容易，故深度學習的觀察指標仍以損失函數/誤差為主
	* 對於不同資料類型，適合加深與加寬的問題都有，但加深適合的問題類型較多
	* 輸入特徵的選擇影響結果甚鉅，因此深度學習也需要考慮特徵工程
	
	* https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-gauss&learningRate=0.3&regularizationRate=0&noise=0&networkShape=7,1&seed=0.28326&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=regression&initZero=false&hideText=false
	* Test lost 0.004: 1 layer 7 neurons
	
	
# D65：深度學習體驗 : 啟動函數與正規化

	* 練習 5：切換批次大小
		* 批次大小很小時，雖然收斂過程非常不穩定，但平均而言會收斂到較好的結果
		* 實務上，批次大小如果極小，效果確實比較好，但計算時間會相當久，因此通常會依照時間需要而折衷
		
	* 練習 6：切換學習速率
		* 0.03 => Training Loss : 0.027
		* 0.1 => Training Lost : 0.006
		* 0.3 => 開始震盪
		* 小於 0.3 時 學習速率較大時，收斂過程會越不穩定，但會收斂到較好的結果
		* 大於 1 時 因為過度不穩定而導致無法收斂
		
	* 練習 7：切換啟動函數
		* 在這種極端的情形下，Tanh 會無法收斂，ReLU 很快就穩定在很糟糕的分類狀態，惟有 Sigmoid 還可以收斂到不錯的結果
		* 但實務上，Sigmoid 需要大量計算時間，而 ReLU 則相對快得很多，這也是需要取捨的，在本例中因位只有一層，所以狀況不太明顯
		
	* 練習 ８：切換正規化選項與參數
		* Regularization : 
			* None: 0.051
			* L1: Test Loss 0.042
			* L2: Test Loss 0.005
		* 我們已經知道上述設定本來來就會收斂，只是在較⼩小的 L1 / L2 正規劃參參數下收斂比較穩定⼀一點
		* 但正規化參數只要略大，反而會讓本來能收斂的設定變得無法收斂，這點 L1 比 L2情況略嚴重，因此本例中最適合的正規化參數是 L2 + 參數 0.001
		* 實務上：L1 / L2 較常使用在非深度學習上，深度學習上效果有限
	
	* 重要知識點複習
		* 批次大小 : 越小, 學習曲線越不穩定、但收斂越快
		* 學習速率: 越大, 學習曲線越不穩定、但收斂越快，但是與批次大小不同的是 - 學習速率大於一定以上時，有可能不穩定到無法收斂
		* 當類神經網路層數不多時，啟動函數 Sigmoid / Tanh 的效果比 Relu 更更好
		* L1 / L2 正規化在非深度學習上效果較明顯，而正規化參數較小才有效果
		
	* Homework
		* Activation
			* ReLU	: 0.010 Test Loss, 0.2x
			* Others: 0.4x
		* Learning Rate: 
			* 0.3 : unstable
			* 0.1 : 0.032, 0.009 Test Loss
		* Regularization
			* None : 0.009
			* L1 : 0.034
			* L2 : 0.008
		* Batch
			* 10 : 0.008, 0.369, 0.096 (Not stable)
			* 5 : 0.172, 0.030, 0.166 (Not stable)
			* 20 : 0.143, 0.040, 0.095 (Stable)
			* 30 : 0.020, 0.051, 0.024
		
# D66：Keras 安裝與介紹全螢幕瀏覽

	* Keras是一個高層神經網絡API，Keras由純Python編寫而成並基
	* Tensorflow、Theano以及CNTK後端。
	* 簡易和快速的原型設計（keras具有高度模塊化，極簡，和可擴充特性）
	* 支持CNN和RNN，或二者的結合
	* 無縫CPU和GPU切換
	* Python協作：Keras沒有單獨的模型配置文件類型（作為對比，caffe有），模型由python代碼描述，使其更緊湊和更易debug，並提供了擴展的便利性
	
	* Final solution to install on my laptop
		* https://medium.com/@johnnyliao/%E5%9C%A8nvidia-mx150%E7%9A%84win10%E5%AE%89%E8%A3%9Dcuda-toolkit-cudnn-python-anaconda-and-tensorflow-91d4c447b60e
		* python 3.5
		* CUDA v9.2 ( https://developer.nvidia.com/cuda-downloads )
		* cuDNN v7.5 ( https://developer.nvidia.com/rdp/cudnn-download )
		* CUDA: 1.10 
			* conda create -n keras36 python=3.6
			* conda activate keras36
			* conda install tensorflow-gpu=1.10
			* conda install keras
			* conda install notebook
			* conda install matplotlib
			* conda install pillow
			* conda install imageio
			* conda install pandas

	* Commands
		* (keras35) C:\Users\erwin>python
			Python 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:05:27) [MSC v.1900 64 bit (AMD64)] on win32
			Type "help", "copyright", "credits" or "license" for more information.
			>>> from tensorflow.python.client import device_lib
			>>> print(device_lib.list_local_devices())
				2019-04-16 00:12:47.227197: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
				2019-04-16 00:12:47.904239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
				name: GeForce MX150 major: 6 minor: 1 memoryClockRate(GHz): 1.5315
				pciBusID: 0000:01:00.0
				totalMemory: 2.00GiB freeMemory: 1.62GiB
				2019-04-16 00:12:47.908523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
				2019-04-16 00:12:48.355539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
				2019-04-16 00:12:48.359741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
				2019-04-16 00:12:48.361894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
				2019-04-16 00:12:48.364882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with 1371 MB memory) -> physical GPU (device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1)
				[name: "/device:CPU:0"
				device_type: "CPU"
				memory_limit: 268435456
				locality {
				}
				incarnation: 1394821717225229750
				, name: "/device:GPU:0"
				device_type: "GPU"
				memory_limit: 1437731635
				locality {
				  bus_id: 1
				  links {
				  }
				}
				incarnation: 10895563417341006762
				physical_device_desc: "device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1"
				]
			>>>
			
	* Keras
		* Getting started: 30 seconds to Keras
			* The core data structure of Keras is a model, a way to organize layers. 
			* The simplest type of model is the Sequential model, a linear stack of layers. 
			* For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers.
		* Here is the Sequential model:
			* from keras.models import Sequential
			* model = Sequential()
		* Stacking layers is as easy as .add():
			* from keras.layers import Dense
			* model.add(Dense(units=64, activation='relu', input_dim=100))
			* model.add(Dense(units=10, activation='softmax'))
		* Once your model looks good, configure its learning process with .compile():
			* model.compile(loss='categorical_crossentropy',
						  optimizer='sgd',
						  metrics=['accuracy'])
		* If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, 
				while allowing the user to be fully in control when they need to 
				(the ultimate control being the easy extensibility of the source code).
			* model.compile(loss=keras.losses.categorical_crossentropy,
						  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))
		* You can now iterate on your training data in batches:
			* # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.
			* model.fit(x_train, y_train, epochs=5, batch_size=32)
		* Alternatively, you can feed batches to your model manually:
			* model.train_on_batch(x_batch, y_batch)
		* Evaluate your performance in one line:
			* loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)
		* Or generate predictions on new data:
			* classes = model.predict(x_test, batch_size=128)
	
	* Keras 程序
		* Step 1: 選擇模型
			* Sequential model
			* 函數式模型
		* Step 2: 構建網路層
			* 網路層以位置來分：
				* 輸入層
				* 隱藏層 x N
				* 輸出層
			* 包含的函數：
				* Activation function
				* Initialization function
				* Regularization 正規項
				* 約束項
			* 每層都可以包括各種網路層
				* 常用層
				* 卷積層
				* 池化層
				* 局部連接層
				* 循環層
				* ...
		* Step 3: 編譯
			* 優化函數
			* Loss 函數
			* 性能評估
		* Step 4: 訓練
			* Callback function
		* Step 5: 預測
			* 數據預處理
				* 序列預處理
				* 文本預處理
				* 圖片預處理
			
			
# D67：Keras Dataset

	* Keras Dataset – CIFAR10
		* CIFAR10 small image classification
		* Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.
	* Keras Dataset – CIFAR100
		* CIFAR100 small image classification
		* Dataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.
	* Keras Dataset – MNIST database
		* MNIST database of handwritten digits
		* Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images
	* Keras Dataset - Fashion-MNIST
		* Fashion-MNIST database of fashion articles
		* Dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST.
	* Keras Dataset - Boston housing price
	* Keras Dataset – IMDB電影評論情緒分類
		* from keras.datasets import imdb
	* Keras Dataset – 路透社新聞專題主題分類
		* from keras.datasets import reuters
		
	* 適用於文本分析與情緒分類
		• IMDB電影評論情緒分類
		• 路透社新聞專題主題分類
		
	* 適用於 Data/Numerical 學習
		• Boston housing price regression dataset
		
	* 適用於影像分類與識別學習
		• CIFAR10/CIFAR100
		• MNIST/ Fashion-MNIST
		
	* 針對小數據集的深度學習
		• 數據預處理與數據提升
		
	* HW: Load CIFAR10 and CIFAR100
	
	* 推薦數據集
		* Imagenet
			* Imagenet數據集有1400多萬幅圖片，涵蓋2萬多個類別；其中有超過百萬的圖片有明確的類別標註和圖像中物體位置的標註，具體信息如下：
				1) Total number of non-empty synsets : 21841
				2）Total number of images: 14,197,122
				3）Number of images with bounding box annotations: 1,034,908
				4）Number of synsets with SIFT features: 1000
				5）Number of images with SIFT features: 1.2 million
			* Imagenet數據集是目前深度學習圖像領域應用得非常多的一個領域，關於圖像分類、定位、檢測等研究工作大多基於此數據集展開。
			* Imagenet數據集文檔詳細，有專門的團隊維護，使用非常方便，在計算機視覺領域研究論文中應用非常廣，幾乎成為了目前深度學習圖像領域算法性能檢驗的“標準”數據集。
			* 數據集大小：~1TB（ILSVRC2016比賽全部數據）下載地址：http://www.image-net.org/about-stats
		* COCO
			* COCO(Common Objects in Context)是一個新的圖像識別、分割和圖像語義數據集，它有如下特點：
				1）Object segmentation 
				2）Recognition in Context 
				3）Multiple objects per image 
				4）More than 300,000 images 
				5）More than 2 Million instances 
				6）80 object categories 
				7）5 captions per image 
				8）Keypoints on 100,000 people
			* COCO數據集由微軟贊助，其對於圖像的標註信息不僅有類別、位置信息，還有對圖像的語義文本描述，
			* COCO數據集的開源使得近兩三年來圖像分割語義理解取得了巨大的進展，也幾乎成為了圖像語義理解算法性能評價的“標準”數據集。
			* Google開源的開源了圖說生成模型show and tell就是在此數據集上測試的，想玩的可以下下來試試。
			* 數據集大小：~40GB 下載地址：http://mscoco.org/

			
# D68：Keras Sequential API

	* Sequential API
		* Way one
			* from keras.models import Sequential
			* from keras.layers import Dense, Activation
			* model = Sequential([Dense(32, _input_shap=(784,)), Activation(“relu”)])
		* 也可以透過 .add
			* model = Sequential()
			* model.add(Dense(32, _input_dim=784))
			* model.add(Activation(“relu”))
			
	* 常用參數說明
		* Dense 實現全連接層
			* Dense(units,activation,use_bias=True, kernel_initializer=’glorot_uniform’, bias_initializer=’zeros’)
		* Activation 對上層輸出應用激活函數
			* Activation(activation)
		* Dropout 對上層輸出應用 dropout 以防止過擬合
			* Dropout(ratio)
		* Flatten 對上層輸出一維化
			* Flatten()
		Reahape 對上層輸出reshape 
			* Reshape(target_shape)
			
	* 重要知識點複習
		* Sequential 序貫模型: 序貫模型為最簡單的線性、從頭到尾的結構順序，一路到底
		Sequential模型的基本元件一般需要：
		• Model 宣告
		• model.add，添加層；
		• model.compile,模型訓練；
		• model.fit，模型訓練參參數設置 + 訓練；
		• 模型評估
		• 模型預測
		
	*Sample codes
		# build our CNN model
		model = Sequential()
		# layer 1
		model.add(Conv2D(filters=64, 
						 kernel_size=(3,3),
						 input_shape=(32, 32,3), 
						 activation='relu', 
						 padding='same'))
		# layer 2
		model.add(MaxPooling2D(pool_size=(2, 2)))
		# layer 3
		model.add(Flatten())
		# layer 4
		model.add(Dense(1024))
		# layer 5
		model.add(Activation('relu'))
		# layer 6
		model.add(Dense(num_classes))
		# lyaer 7
		model.add(Activation('softmax'))
		# Dump all model layers
		print(model.summary())
		
				
# D69：Keras Module API
		
	* 函數式模型是最廣泛的一類模型，序貫模型（Sequential）只是它的一種特殊情況
		
	* 輸入是張量，輸出也是張量的一個框架就是一個模型，通過Model定義


# D70：Multi-layer Perception多層感知

	* 多層感知機其實就是可以用多層多個perception 來達到最後目的
		• 在機器學習領域像是我們稱為 multiple classification system 或是 ensemble learning
	* 深度神經網路路(deep neural network, DNN)，神似人工神經網路的MLP
	* 若每個神經元的激活函數都是線性函數，那麼，任意層數的MLP都可被約簡成一個等價的單層感知器

	* 機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP)運作方式
		* https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E9%81%8B%E4%BD%9C%E6%96%B9%E5%BC%8F-f0e108e8b9af
		* 由ＸＯＲ問題的例子可以知道，第一層兩個Perception在做的事情其實是將資料投影到另一個特徵空間去
		*（這個特徵空間大小是根據你設計的Perception數目決定的），
		* 所以最後再把h1和h2的結果當作另一個Perception的輸入，再做一個下一層的Perception就可以完美分類XOR問題啦


# D71：損失函數

	* 損失函數大致可分為：分類問題的損失函數和回歸問題的損失函數
	
	* 損失函數為什麼是最小化
		* 期望: 希望模型預測出來的東西可以跟實際的值一樣
		* 預測出來的東西基本上跟實際值都會有落差
			• 在回歸問題稱為「殘差(residual)」
			• 在分類問題稱為「錯誤率(error rate)」
		* 損失函數中的損失就是「實際值和預測值的落差」
		
	* 損失函數的分類介紹 - mean_squared_error
		* 均方誤差(mean_squared_error)：就是最小平方法(Least Square) 的目標函數 -- 預測值與實際值的差距之平均值。
			* 還有其他變形的函數, 如 mean_absolute_error、mean_absolute_percentage_error、mean_squared_logarithmic_error。
		* 使用時機：
			• n個樣本的預測值（y）與（y_）的差距
			• Numerical 相關
		* Keras 上的調用方式：
			• from keras import losses
			• model.compile(loss= ‘mean_squared_error‘, optimizer='sgd’)
			• 其中，包含y_true， y_pred的傳遞，函數是表達如下：
			• keras.losses.mean_squared_error(y_true, y_pred)
		
	* 損失函數的分類介紹 - Cross Entropy
		* 當預測值與實際值愈相近，損失函數就愈小，反之差距很大，就會更影響損失函數的值
		* 要用 Cross Entropy 取代 MSE，因為，在梯度下時，Cross Entropy 計算速度較快 						???
		* 使用時機：
			• 整數目標：Sparse categorical_crossentropy
			• 分類目標：categorical_crossentropy
			• 二分類目標：binary_crossentropy。
		* Keras 上的調用方式：
			• from keras import losses
			• model.compile(loss= ‘categorical_crossentropy ‘, optimizer='sgd’)
			• 其中, 包含y_true， y_pred的傳遞, 函數是表達如下：
			• keras.losses.categorical_crossentropy(y_true, y_pred)
			
	* 損失函數的分類介紹: Hinge Error (hinge) 																???
		* 是一種單邊誤差，不考慮負值同樣也有多種變形，squared_hinge、categorical_hinge
		* 使用時機：
			• 適用於『支援向量機』(SVM)的最大間隔分類法(maximum-margin classification)
		* Keras 上的調用方式：
			• from keras import losses
			• model.compile(loss= ‘hinge‘, optimizer='sgd’)
			• 其中，包含y_true，y_pred的傳遞, 函數是表達如下:
			• keras.losses.hinge(y_true, y_pred)
			
	* 特別的案例: 自定義損失函數 																			???
		* 根據問題的實際情況，定制合理的損失函數
		* 舉例：預測果汁日銷量問題，如果預測銷量大於實際銷量則會損失成本；如果預測銷量小於實際銷量則會損失利潤。
		• 考慮重點：製造一盒果汁的成本和銷售一盒果汁的利利潤不是等價的
		• 需要使用符合該問題的自定義損失函數自定義損失函數為 
			• 損失函數表示若預測結果y小於標準答案y_，損失函數為利潤乘以預測結果y與標準答案之差
			• 若預測結果y大於標準答案y_，損失函數為成本乘以預測結果y與標準答案之差用
		• Tensorflow函數表⽰示為：
			loss = tf.reduce_sum(tf.where(tf.greater(y, y_), COST*(y-y_), PROFIT*(y_-y)))
		
	* 複習 : 流程
		* 損失函數中的損失就是「實際值和預測值的落差」，損失函數是最小化
		* 損失函數大致可分為：分類問題的損失函數和回歸問題的損失函數
	
	
# D72：激活函數

	* 完成今日課程後你應該可以了解 : 針對不同的問題使用合適的激活函數
	
	* 激活函數的作用
		* 深度學習的基本原理理是基於人工神經網絡，信號從一個神經元進入，經過非線性的 activation function
			• 如此循環往復，直到輸出層。正是由於這些非線性函數的反复疊加，才使得神經網絡有足夠的 capacity 來來抓取複雜的 pattern		
		* 激活函數的最大作用就是非線性化
			• 如果不用激活函數的話，無論神經網絡有多少層，輸出都是輸入的線性組合
		* 激活函數的另一個重要特徵是：它應該是可以區分
			• 以便在網絡中向後推進以計算相對於權重的誤差（丟失）梯度時執行反向優化策略，
			* 然後相應地使用梯度下降或任何其他優化技術優化權重以減少誤差
	
	* 常用激活函數介紹：Sigmoid
		* 特點是會把輸出限定在 0~1 之間，
			* 在 x<0 ，輸出就是 0，
			* 在 x>0，輸出就是 1，
			* 這樣使得數據在傳遞過程中不容易發散
		* 兩個主要缺點
			• 一是 Sigmoid 容易過飽和，丟失梯度。這樣在反向傳播時，很容易出現梯度消失的情況，導致訓練無法完整
			• 二是 Sigmoid 的輸出均值不是 0
			
	* 常用激活函數介紹：Softmax
		* 輸出數值介於 0 ~ 1 之間
		* Softmax 把一個 k 維的 real value 向量（a1,a2,a3,a4….）映射成一個（b1,b2,b3,b4….）
			* 其中 bi 是一個 0～1 的常數，輸出神經元之和為1.0，所以可以拿來做多分類的機率預測
		* 為什麼要取指數
			• 第一個原因是要模擬 max 的行為，所以要讓大的更大。
			• 第二個原因是需要一個可導的函數
			
	* 常用激活函數介紹：Tanh
		* 輸出數值介於 -1 ~ 1 之間
		* tanh 讀作 Hyperbolic Tangent tanh 也稱為雙切正切函數，取值範圍為 [-1,1]。
		* tanh 在特徵相差明顯時的效果會很好，在循環過程中會不斷擴大特徵效果
			
	* 常用激活函數介紹：Sigmoid vs Softmax
		* Sigmoid 將一個 real value 映射到（0,1）的區間，用來做二分類
		* Softmax 把一個 k 維的 real value 向量量（a1,a2,a3,a4….）映射成一個（b1,b2,b3,b4….）
			* 其中 bi 是一個 0～1 的常數，輸出神經元之和為 1.0，所以可以拿來來做多分類的機率預測
		* 二分類問題時 sigmoid 和 softmax 是一樣的，求的都是 cross entropy loss
			
	* 常用激活函數介紹: Sigmoid vs Tanh
		* tanh 函數將輸入值壓縮到 -1~1 的範圍，因此它是 0 均值的，解決了Sigmoid 函數的非 zero-centered 問題，
		* 但是它也存在梯度消失和冪運算的問題。
		* 其實 tanh(x)=2sigmoid(2x)-1
		
	* 常用激活函數介紹：ReLU
		* 輸出數值介於 0 ~ infinity 之間
			* f(x) = max(0, x)
		* 修正線性單元（Rectified linear unit，ReLU）
			• 在 x>0 時導數恆為1
			• 對於 x<0，其梯度恆為 0，這時候它也會出現飽和的現象，甚至使神經元直接無效，從而其權重無法得到更新（在這種情況下通常稱為 dying ReLU）
			* Leaky ReLU 和 PReLU 的提出正是為了解決這一問題
		* ELU 函數是針對 ReLU 函數的一個改進型，相比於 ReLU 函數，在輸入為負數的情況下，是有一定的輸出的
			* f(x) = 
				* x if x > 0
				* a((e power of x) - 1)
		* 這樣可以消除 ReLU 死掉的問題
		* 還是有梯度飽和和指數運算的問題
		* PReLU
			* f(x) = max(ax, x)
			• 參數化修正線性單元（Parameteric Rectified Linear Unit，PReLU）屬於 ReLU 修正類激活函數的一員。
		* Leaky ReLU
			• 當 α=0.1 時，我們叫 PReLU 為 Leaky ReLU，算是 PReLU 的一種特殊情況
		* RReLU 以及 Leaky ReLU 有一些共同點，即爲負值輸入添加了一個線性項。
		
	* 常用激活函數介紹: Maxout
		* f(x) = max(wT1x + b1, wT2x + b2)
		* Maxout 是深度學習網絡中的一層網絡，就像池化層、卷積層一樣，可以看成是網絡的激活函數層
		* Maxout 神經元的激活函數是取得所有這些「函數層」中的最大值
		* Maxout 的擬合能力是非常強的，優點是計算簡單，不會過飽和，同時又沒有 ReLU 的缺點
		* Maxout 的缺點是過程參數相當於多了一倍
		
	* 如何選擇正確的激活函數
		* 根據各個函數的優缺點來來配置
			• 如果使用 ReLU，要小心設置 learning rate，注意不要讓網絡出現很多「dead」 神經元，
			* 如果不好解決，可以試試 Leaky ReLU、PReLU 或者 Maxout
		* 根據問題的性質
			• 用於分類器時，Sigmoid 函數及其組合通常效果更更好
			• 由於梯度消失問題，有時要避免使用 sigmoid 和 tanh 函數。
			* ReLU 函數是一個通用的激活函數，目前在大多數情況下使⽤用
			• 如果神經網絡中出現死神經元，那麼 PReLU 函數就是最好的選擇
			• ReLU 函數建議只能在隱藏層中使⽤用
		* 如何得知 「dead」 神經元 																			???
		* 考慮 DNN 損失函數和激活函數
			• 如果使用 sigmoid 激活函數，則交叉熵損失函數一般肯定比均方差損失函數好；
			• 如果是 DNN 用於分類，則一般在輸出層使用 softmax 激活函數
			• ReLU 激活函數對梯度消失問題有一定程度的解決，尤其是在 CNN 模型中。
		* 梯度消失 Vanishing gradient problem
			* 原因：前面的層比後面的層梯度變化更小，故變化更慢
			* 結果：Output 變化慢 => Gradient 小 => 學得慢
			* Sigmoid，Tanh 都有這樣特性不適合用在 Layers 多的DNN 架構
		
	* 前述流程 / python程式 對照
		* 激活函數可以通過設置單獨的激活層實現，可以也在構造層對象時通過傳遞 activation 參數實現：
			* from keras.layers import Activation,Dense 
			* model.add(Dense(64,activation=‘tanh’))
		* 考慮不同Backend support，通過傳遞一個元素運算的Theano / TensorFlow / CNTK函數來作為激活函數：
			* from keras import backend as k
			* model.add(Dense(64,activation=k.tanh))
			* model.add( Activation(k.tanh))
		
	* 重要知識點複習：何謂激活函數 (Activation Function)
		* 激活函數定義了每個節點（神經元）的輸出和輸入關係的函數為神經元提供規模化非線性化能力，讓神經網絡具備強大的擬合能力
		
	* 重要知識點複習：如何選擇正確的激活函數
		* 根據各個函數的優缺點來配置
			• 如果使用 ReLU，要小心設置 learning rate，注意不要讓網絡出現很多「dead」 神經元，
			* 如果不好解決，可以試試 Leaky ReLU、PReLU 或者 Maxout
		* 根據問題的性質
			• 用於分類器時，Sigmoid 函數及其組合通常效果更更好
			• 由於梯度消失問題，有時要避免使用 sigmoid 和 tanh 函數。
			* ReLU 函數是一個通用的激活函數，目前在大多數情況下使用
			• 如果神經網絡中出現死神經元，那麼 PReLU 函數就是最好的選擇
			• ReLU 函數建議只能在隱藏層中使⽤用
		
	* 各種 Activation 适用范围
		* Sigmoid 函数：主要用于二分类的输出层，否则不用。
		* tanh 函数：几乎所有场合均可以。 
			* 個人覺得都很差
		* ReLU(Rectified Linear Unit 修正的线性单元)：二分类问题选择 sigmoid ，余下的问题 ReLU 是默认的选择
		* leaky ReLU（带泄露的ReLU）
		
		
			
			
			
			
# D101-D105：影像辨識

	* 